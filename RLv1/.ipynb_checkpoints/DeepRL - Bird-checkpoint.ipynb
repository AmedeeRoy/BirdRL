{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, Flatten, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as kerasBackend\n",
    "from ple import PLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlbirdv1 import *\n",
    "\n",
    "x = [i for i in range(3)]\n",
    "y = [i for i in range(3)]\n",
    "y.reverse()\n",
    "\n",
    "# List of coordinates for islands\n",
    "island = [(0, 0)]\n",
    "birdStart = (0, 0)\n",
    "\n",
    "TILESIZE = 40\n",
    "SCREEN_WIDTH = TILESIZE*(len(x)+3)\n",
    "SCREEN_HEIGHT = TILESIZE * len(y)\n",
    "\n",
    "reward = {'lose' : -1000,\n",
    "          'win' : 1000}\n",
    "\n",
    "listAction = [K_LEFT, K_RIGHT, K_DOWN, K_UP, K_s, K_d]\n",
    "\n",
    "N = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon\n",
    "INITIAL_EXPLORATION = 200\n",
    "EXPLORATION_STEPS = 800\n",
    "INITIAL_EPSILON = 1\n",
    "FINAL_EPSILON = 1e-3\n",
    "\n",
    "# Constants\n",
    "GAMMA = 0.99\n",
    "NUMBER_GAMES = 2000\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "def epsilon(step):\n",
    "    \"\"\"\n",
    "    Epsilon for exploration/exploitation trade-off\n",
    "    \"\"\"\n",
    "    if step < INITIAL_EXPLORATION:\n",
    "        return 1\n",
    "    elif step < EXPLORATION_STEPS:\n",
    "        return INITIAL_EPSILON + (FINAL_EPSILON - INITIAL_EPSILON)/(EXPLORATION_STEPS-INITIAL_EXPLORATION) * (step-INITIAL_EXPLORATION)\n",
    "    else:\n",
    "        return FINAL_EPSILON\n",
    "    \n",
    "def createDQN():\n",
    "    \"\"\"\n",
    "    Create deep Q network\n",
    "    \"\"\"\n",
    "    # Neural network\n",
    "    dqn = Sequential()\n",
    "    dqn.add(Dense(units = 6, input_dim = 4 , activation='relu'))\n",
    "    dqn.add(Activation('softmax'))\n",
    "\n",
    "#     dqn = Sequential()\n",
    "#     dqn.add(Conv2D(filters=16, kernel_size=(8,8), strides=4, activation='relu', input_shape=(80,80,4)))\n",
    "#     dqn.add(Conv2D(filters=32, kernel_size=(4,4), strides=2, activation='relu'))\n",
    "#     dqn.add(Flatten())\n",
    "#     dqn.add(Dense(units=256, activation='relu'))\n",
    "#     dqn.add(Dense(units=2, activation='linear'))\n",
    "    \n",
    "    dqn.compile(optimizer=Adam(lr=LEARNING_RATE), loss='mean_squared_error')\n",
    "    return dqn\n",
    "\n",
    "\n",
    "def epsilonGreedy(dqn, x, step):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon(step):\n",
    "        return np.random.randint(6)\n",
    "    else:\n",
    "        return np.argmax(dqn.predict(np.array([x])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load DQN, or create a new one\n",
    "dqn = createDQN()\n",
    "\n",
    "# Environment\n",
    "game = RLBird(width = SCREEN_WIDTH, height = SCREEN_HEIGHT, x = x, y = y,\\\n",
    "              init_bird_position = birdStart, island_position = island, \\\n",
    "              energyMax = 10, catchMax = 2, costMove = -1, costDive = -1, gainFish = 2, factorFishFly = 0.75,\\\n",
    "              nbStep = N, reward = reward)\n",
    "p = PLE(game, fps=100, frame_skip=1, num_steps=1, force_fps=False, display_screen=True, reward_values = reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "p.init()\n",
    "p.reset_game()\n",
    "saveFishMap = game.map.fishMap.copy()\n",
    "\n",
    "for k in range(NUMBER_GAMES):\n",
    "    \n",
    "    p.init()\n",
    "    p.reset_game()\n",
    "    game.updateFishMap(saveFishMap.copy())\n",
    "    reward = 0.0\n",
    "    \n",
    "    while(not p.game_over()):\n",
    "        state = game.getGameState()\n",
    "        x = game.listStates.state2idx(np.array(state))\n",
    "        \n",
    "        a = epsilonGreedy(dqn, state, k)\n",
    "        reward = p.act(listAction[a])\n",
    "        \n",
    "        state_new = game.getGameState()\n",
    "        x_new = game.listStates.state2idx(np.array(state_new))\n",
    "        \n",
    "        \n",
    "        Q  = dqn.predict(np.array([state]))\n",
    "        Q_new = dqn.predict(np.array([state_new]))\n",
    "        \n",
    "        \n",
    "        if p.game_over():\n",
    "            update = reward \n",
    "        else :\n",
    "            update = reward + GAMMA * np.max(Q_new)\n",
    "        Q[0,a] = update\n",
    "       \n",
    "        \n",
    "        dqn.train_on_batch(np.array([state]),Q)\n",
    "\n",
    "        \n",
    "        \n",
    "# Q-learning's final value function and policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
